{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "To build a neural net from scratch, we need to go over each block and code those individually. At the end we can combine all of these to create an $L$-layer NN.\n",
    "\n",
    "So, the steps we need to take are:\n",
    "<ul>\n",
    "    <li>Parameter Intialization: We need to initialize parameters $W$ and $b$</li>\n",
    "    <li>Compute a forward propagation pass: This involves computing the linear pass - $Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$ - and the activation $A^{[l]}=g(Z^{[l]})$ for both Sigmoid and ReLU activations</li>\n",
    "    <li>Compute the loss</li>\n",
    "    <li>Implement a back propagation pass</li>\n",
    "    <li>Update the parameters: Here I'll code in mini Batch Gradient Descent (Which will cover both Stochastic Gradient Descent as well as Batch Gradient Descent), Momentum, RMSProp, and the king of them all, Adam</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "To add non-linearity to the model, activation functions are used. I'll define them now.\n",
    "I'll be using ReLU (rectified linear unit) and sigmoid in an example, but I'll also define tanh and leaky ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Z -- output of linear function Z = W*A+b\n",
    "    \n",
    "    Returns:\n",
    "    ret -- ReLU(Z)\n",
    "    Z -- input for use in backprop\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.maximum(0,Z), Z\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Z -- output of linear function Z = W*A+b\n",
    "    \n",
    "    Returns:\n",
    "    ret -- sigmoid(Z)\n",
    "    Z -- input for use in backprop\n",
    "    \n",
    "    \"\"\"\n",
    "    return 1./(1.+np.exp(-Z)), Z\n",
    "\n",
    "def tanh(Z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Z -- output of linear function Z = W*A+b\n",
    "    \n",
    "    Returns:\n",
    "    ret -- tanh(Z)\n",
    "    Z -- input for use in backprop\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.tanh(Z), Z\n",
    "\n",
    "def leaky_relu(Z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Z -- output of linear function Z = W*A+b\n",
    "    \n",
    "    Returns:\n",
    "    ret -- leaky_relu(Z)\n",
    "    Z -- input for use in backprop\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.maximum(0.01*Z, Z), Z\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Initialization\n",
    "For passing parameter information between different functions, I'll use a dictionary `parameters`, which will store $W$ and $b$ values for each layer $l \\{l:{0\\le l \\le L}\\}$\n",
    "\n",
    "Additionally, I'll implement random, Xavier initialization, and He initialization.\n",
    "\n",
    "<ul>\n",
    "    <li>Random Initialization: Samples values from a normal distribution, and multiplies by a small value to keep weights close to zero - regularization</li>\n",
    "    <li>Xavier Initialization: random sampling is multiplied by constant $\\sqrt{\\frac{1}{\\text{previous layer dimension}}}$</li>\n",
    "    <li>He Initialization: random sampling is multiplied by constant $\\sqrt{\\frac{2}{\\text{previous layer dimension}}}$</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_shape, initialization_method='he'):\n",
    "    \"\"\"\n",
    "    Initializes parameters W and b of a network of shape layer_shape.\n",
    "    \n",
    "    Arguments:\n",
    "    layer_shape -- list containing the dimensions of each network layer l\n",
    "    \n",
    "    Returns:\n",
    "    parameters --  dictionary containing weight and bias parameters\n",
    "    \"\"\"\n",
    "    #define dictionary\n",
    "    params = {}\n",
    "    \n",
    "    #Obtain L\n",
    "    L = len(layer_shape)\n",
    "    \n",
    "    #Check initialization_method\n",
    "    if initialization_method == 'random':\n",
    "        beta = 0.01\n",
    "        for l in range(1,L):\n",
    "            params[\"W\"+str(l)] = np.random.randn(layer_shape[l], layer_shape[l-1])*beta\n",
    "            params[\"b\"+str(l)] = np.zeros([layer_shape[l], 1])\n",
    "    \n",
    "    elif initialization_method == 'xavier':\n",
    "        for l in range(1,L):\n",
    "            beta = np.sqrt(1./layer_shape[l-1])\n",
    "            params[\"W\"+str(l)] = np.random.randn(layer_shape[l], layer_shape[l-1])*beta\n",
    "            params[\"b\"+str(l)] = np.zeros([layer_shape[l], 1])\n",
    "    \n",
    "    elif initialization_method == 'he':\n",
    "        for l in range(1,L):\n",
    "            beta = np.sqrt(2./layer_shape[l-1])\n",
    "            params[\"W\"+str(l)] = np.random.randn(layer_shape[l], layer_shape[l-1])*beta\n",
    "            params[\"b\"+str(l)] = np.zeros([layer_shape[l], 1])\n",
    "    else:\n",
    "        raise NameError(\"%s is not a valid initalization method\"%(initialization_method))\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "Forward propagation refers to passing through the computation graph from left to right - forwards - and evaluating $Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$ for each sucessive $l$ starting with $l=1$, in which case $A^{[0]}=X$, in other words, the activation fed into the first layer is simply the inputs.\n",
    "\n",
    "To accomplish this, I'll create two functions. The first will evaluate the linear formula $Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$, whereas the second will evaluate $A^{[l]} = g(Z^{[l]})$, which corresponds to evaluating the activation function.\n",
    "\n",
    "Then `forward_prop` implements both to complete a forward propagation pass.\n",
    "\n",
    "In order to compute the backprop later onwards, I'll need to store $A^{[l]}$,$W^{[l]}$, $b^{[l]}$ as well as $Z^{[l]}$ which I'll do in `linear cache` and `activation cache`\n",
    "\n",
    "One of the arguments of `forward_prop` is `layer_activations`, which is a list of the activations for each layer of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_linear(W,A,b):\n",
    "    \"\"\"\n",
    "    Linear part of forward propagation\n",
    "\n",
    "    Arguments:\n",
    "    W -- weight matrix\n",
    "    A -- activations\n",
    "    b -- bias matrix\n",
    "\n",
    "    Returns:\n",
    "    Z -- input to the layer's activation function\n",
    "    linear_cache -- tuple with A, W, b for efficient backprop\n",
    "    \"\"\"\n",
    "    Z = np.dot(W,A)+b\n",
    "    \n",
    "    linear_cache = (A,W,b)\n",
    "    \n",
    "    return Z, linear_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_activation(Z, activation):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Z -- Output of linear function Z = WA_prev+b\n",
    "    activation -- String denoting activation function to use. One of [linear, sigmoid, relu, leaky_relu, tanh]\n",
    "    \n",
    "    Returns:\n",
    "    A -- g(Z), where g() is the corresponding activation\n",
    "    activation_cache -- the input Z, which will be fed into backprop\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == 'linear':\n",
    "        A, activation_cache = Z, Z\n",
    "    elif activation == 'sigmoid':\n",
    "        A, activation_cache = sigmoid(Z), Z\n",
    "    elif activation == 'relu':\n",
    "        A, activation_cache = relu(Z), Z\n",
    "    elif activation == 'leaky_relu':\n",
    "        A, activation_cache = leaky_relu(Z), Z\n",
    "    elif activation == 'tanh':\n",
    "        A, activation_cache = tanh(Z), Z\n",
    "    else:\n",
    "        raise NameError('%s is not a valid activation function' %(activation))\n",
    "    \n",
    "    return A, activation_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, layer_activations, parameters):\n",
    "    \"\"\"\n",
    "    Implements one pass of forward propagation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data\n",
    "    layer_activations -- list of strings corresponding to the activations of each layer\n",
    "    parameters -- output of initialize_parameters\n",
    "    \n",
    "    Returns:\n",
    "    A - Output of activation function of the last layer\n",
    "    caches - list of caches containing both linear and activation caches\n",
    "    \"\"\"\n",
    "    #Define caches\n",
    "    caches = []\n",
    "    #A[0] is the input\n",
    "    A = X\n",
    "    L = len(parameters)//2 \n",
    "    \n",
    "    for l in range(1, L+1):\n",
    "        A_prev = A\n",
    "        W = parameters[\"W\"+str(l)]\n",
    "        b = parameters[\"b\"+str(l)]\n",
    "        Z, linear_cache = forward_linear(W, A_prev, b)\n",
    "        A, activation_cache = forward_activation(Z, layer_activations[l])\n",
    "        \n",
    "        #Add both linear and activation cache to caches\n",
    "        caches.append((linear_cache, activation_cache))\n",
    "\n",
    "    return A, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "The cost function is the metric that a neural net aims to minimize. I'll implement cross-entropy cost, given by:\n",
    "\n",
    "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$\n",
    "\n",
    "Thus, we require a method of computing cost after one pass of forward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(A_last, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A_last -- Post-activation value of the last layer of the network\n",
    "    Y -- Groud truth vectors\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    #Get number of samples, m\n",
    "    m = Y.shape[1]\n",
    "    #Compute cross entropy cost\n",
    "    cost = -1/m*np.sum(np.multiply(Y, np.log(AL))+np.multiply(1-Y, np.log(1-AL)))\n",
    "    #Ensure appropriate dimensions\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation\n",
    "To update our parameters, we need to calculate the gradient of the loss with respect to $W$ and $b$\n",
    "\n",
    "Just like with forward prop, I will implement two functions. One deals with the back pass for the linear part of the units and the other deals with the derivatives of the activation functions.\n",
    "\n",
    "For the linear part, we take the derivatives of the parameters, obtaining:\n",
    "\n",
    "$$ dW^{[l]} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$\n",
    "$$ db^{[l]} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = W^{[l] T} dZ^{[l]} $$\n",
    "\n",
    "For the activation part, the backprop requires the gradient of the activation function. As such it depends on the activation used, and I'll define them for each one.\n",
    "\n",
    "For sigmoid:\n",
    "\n",
    "$$ \\sigma{(z)} = \\frac{1}{1+e^{-x}}$$\n",
    "$$\\frac{d\\sigma{(z)}}{dz} = \\sigma{(z)}(1-\\sigma{(z)})$$\n",
    "\n",
    "For ReLU:\n",
    "\n",
    "$$\\text{ReLU}(z) = \\max{(0,z)}$$\n",
    "$$\\frac{d\\text{ReLU}}{dz} = \\left\\{\\begin{array}{ll}1 , z > 0\\\\0, z \\le 0\\end{array}\\right.$$\n",
    "\n",
    "Note that for ReLU, strictly speaking, there is a discontinuity at $z=0$, however since it is incredibly unlikely that the input to the function will every be exactly zero, it's fine to include it in  $z\\le0$\n",
    "\n",
    "For tanh:\n",
    "$$\\tanh{(z)} = \\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$$\n",
    "$$\\frac{d\\tanh(z)}{dz} = 1-\\tanh^2(z)$$\n",
    "\n",
    "For leaky ReLU:\n",
    "$$\\text{leaky ReLU}(z) = \\max(0.01z, z)$$\n",
    "$$\\frac{d(\\text{leaky Relu}(z))}{dz} = \\left\\{\\begin{array}{ll}1 , z > 0\\\\0.01, z \\le0\\end{array}\\right.$$\n",
    "\n",
    "So, I'll implement functions for each of these units to compute:\n",
    "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$\n",
    "\n",
    "Additionally, to initialize backpropagation, we need $\\frac{d\\mathcal{L}}{dA^{[L]}}$, the gradient of the cost function with respect to the last activation output. For cross-entropy this is:\n",
    "$$-\\sum\\limits_{i=1}^{m}\\frac{y^{i}}{a^{[L](i)}} - \\frac{1-y^{i}}{1-a^{[L](i)}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_linear(dZ, cache):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dZ -- Gradient of cost w.r.t linear portion\n",
    "    cache -- tuple coming from cached forward prop of layer l\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient with respect to activation of previous layer\n",
    "    dW -- gradient with respect to weights of current layer\n",
    "    db -- gradient with respect to biases of current layer\n",
    "    \"\"\"\n",
    "    \n",
    "    #unpack cache\n",
    "    A_prev, W, b = cache\n",
    "    \n",
    "    #Get number of samples\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = 1/m*np.dot(dZ, A_prev.T)\n",
    "    db = 1/m*np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_activation(dA, Z, activation):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    Z -- cached matrix from forward prop\n",
    "    activation -- the activation to be used in the layer\n",
    "    \n",
    "    Returns:\n",
    "    dZ -- gradient of cost function with respect to Z[l]\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == 'linear':\n",
    "        dZ = dA\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        ret = np.copy(Z)\n",
    "        ret[ret>0] = 1.\n",
    "        ret[ret <= 0] = 0\n",
    "        dZ = np.multiply(dA, ret)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = np.multiply(dA, sigmoid(Z)*(1-sigmoid(Z)))\n",
    "\n",
    "    elif activation == \"leaky_relu\":\n",
    "        ret = np.copy(Z)\n",
    "        ret[ret>0] = 1.\n",
    "        ret[ret <= 0] = 0.01\n",
    "        dZ = np.multiply(dA, ret)\n",
    "\n",
    "    elif activation == \"tanh\":\n",
    "        dZ = np.multiply(dZ, 1 - tanh(Z)**2)\n",
    "    \n",
    "    else:\n",
    "        raise NameError(\"%s is not a valid activation function\" % (activation))\n",
    "\n",
    "    return dZ\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(AL, Y, caches, layer_activations):\n",
    "    \"\"\"\n",
    "    Implement a backward propagation pass\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- output of the forward propagation\n",
    "    Y -- ground truth\n",
    "    caches -- list of caches containing linear_cache and activation_cache\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients dA[l], dW[l], db[l]\n",
    "    \"\"\"\n",
    "    \n",
    "    #Define dict to store gradients for parameter update\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    #Ensure Y is the same as AL (which is essentially y_hat)\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    #Initialize backprop, a.k.a derivative of cost with respect to AL\n",
    "    dAL =  -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    grads[\"dA\"+str(L)] = dAL\n",
    "\n",
    "    for l in reversed(range(L)):\n",
    "        current_cache = caches[L-1]\n",
    "        dA_prev, dW, db = backward_prop(grads[\"dA\"+str(l+1)],current_cache, layer_activations[l])\n",
    "        grads[\"dA\" + str(l)] = dA_prev\n",
    "        grads[\"dW\" + str(l + 1)] = dW\n",
    "        grads[\"db\" + str(l + 1)] = db\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters\n",
    "The final step is to take the gradients computed in back propagation and use them to update the parameters $W$ and $b$.\n",
    "\n",
    "The method of updating these parameters is important and there are several optimizers that do this in different ways.\n",
    "\n",
    "Mini-Batch Gradient Descent:\n",
    "$$ W:=W-\\alpha dW $$\n",
    "$$ b:=b-\\alpha db $$\n",
    "\n",
    "Momentum:\n",
    "\n",
    "RMSProp:\n",
    "\n",
    "Adam:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
